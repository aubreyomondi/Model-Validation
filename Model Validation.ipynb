{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation allows analysts to confidently answer the question, how good is your model?\n",
    "\n",
    "![what_is_model_validation](what_is_model_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seen vs. unseen data\n",
    "\n",
    "Model's tend to have higher accuracy on observations they have seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create vectors of predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Train/Test Errors\n",
    "train_error = mae(y_true=y_train, y_pred=train_predictions)\n",
    "test_error = mae(y_true=y_test, y_pred=test_predictions)\n",
    "\n",
    "# Print the accuracy for seen and unseen data\n",
    "print(\"Model error on seen data: {0:.2f}.\".format(train_error))\n",
    "print(\"Model error on unseen data: {0:.2f}.\".format(test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Model error on seen data: 3.28.\n",
    "\n",
    "Model error on unseen data: 11.07.\n",
    "\n",
    "When models perform differently on training and testing data, you should look to model validation to ensure you have the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Classification\n",
    "\n",
    "Regression - continuous variables.\n",
    "\n",
    "Classification - categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of trees\n",
    "rfr.n_estimators = 100\n",
    "\n",
    "# Add a maximum depth\n",
    "rfr.max_depth = 6\n",
    "\n",
    "# Set the random state\n",
    "rfr.random_state = 1111\n",
    "\n",
    "# Fit the model\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have updated parameters after the model was initialized. \n",
    "\n",
    "This approach is helpful when you need to update parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using X and y\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print how important each column is to the model\n",
    "for i, item in enumerate(rfr.feature_importances_):\n",
    "      # Use i and item to print out the feature importance of each column\n",
    "    print(\"{0:s}: {1:.2f}\".format(X_train.columns[i], item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "    \n",
    "chocolate: 0.44\n",
    "\n",
    "fruity: 0.03\n",
    "\n",
    "caramel: 0.02\n",
    "\n",
    "peanutyalmondy: 0.05\n",
    "\n",
    "nougat: 0.01\n",
    "\n",
    "crispedricewafer: 0.03\n",
    "\n",
    "hard: 0.01\n",
    "\n",
    "bar: 0.02\n",
    "\n",
    "pluribus: 0.02\n",
    "\n",
    "sugarpercent: 0.17\n",
    "\n",
    "pricepercent: 0.19   \n",
    "\n",
    "No surprise here - chocolate is the most important variable. \n",
    "\n",
    ".feature_importances_ is a great way to see which variables were important to your random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Predictions\n",
    "\n",
    "In this exercise, you look at the methods, .predict() and .predict_proba() using the tic_tac_toe dataset. The first method will give a prediction of whether Player One will win the game, and the second method will provide the probability of Player One winning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the rfc model. \n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Create arrays of predictions\n",
    "classification_predictions = rfc.predict(X_test)\n",
    "probability_predictions = rfc.predict_proba(X_test)\n",
    "\n",
    "# Print out count of binary predictions\n",
    "print(pd.Series(classification_predictions).value_counts())\n",
    "\n",
    "# Print the first value from probability_predictions\n",
    "print('The first predicted probabilities are: {}'.format(probability_predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "1    563\n",
    "\n",
    "0    204\n",
    "\n",
    "dtype: int64\n",
    "\n",
    "The first predicted probabilities are: [0.26524423 0.73475577]\n",
    "\n",
    "You can see there were 563 observations where Player One was predicted to win the Tic-Tac-Toe game. Also, note that the predicted_probabilities array contains lists with only two values because you only have two possible responses (win or lose).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Model Parameters\n",
    "\n",
    "Replicating model performance is vital in model validation. Replication is also important when sharing models with co-workers, reusing models on new data or asking questions on a website such as Stack Overflow. You might use such a site to ask other coders about model errors, output, or performance. The best way to do this is to replicate your work by reusing model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Print the classification model\n",
    "print(rfc)\n",
    "\n",
    "# Print the classification model's random state parameter\n",
    "print('The random state is: {}'.format(rfc.random_state))\n",
    "\n",
    "# Print all parameters\n",
    "print('Printing the parameters dictionary: {}'.format(rfc.get_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "            oob_score=False, random_state=1111, verbose=0,\n",
    "            warm_start=False)\n",
    "            \n",
    "The random state is: 1111\n",
    "\n",
    "Printing the parameters dictionary: {'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 1111, 'verbose': 0, 'warm_start': False}\n",
    "\n",
    "Recalling which parameters were used will be helpful going forward. Model validation and performance rely heavily on which parameters were used, and there is no way to replicate a model without keeping track of the parameters used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "\n",
    "This exercise reviews the four modeling steps discussed throughout this chapter using a random forest classification model. You will:\n",
    "\n",
    "1. Create a random forest classification model.\n",
    "2. Fit the model using the tic_tac_toe dataset.\n",
    "3. Make predictions on whether Player One will win (1) or lose (0) the current game.\n",
    "4. Finally, you will evaluate the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=6, random_state=1111)\n",
    "\n",
    "# Fit rfc using X_train and y_train\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Create predictions on X_test\n",
    "predictions = rfc.predict(X_test)\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Print model accuracy using score() and the testing data\n",
    "print(rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "    \n",
    "[1 1 1 1 1]\n",
    "\n",
    "0.817470664928292"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "![train_test](train_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_validation_test](train_validation_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables using pandas\n",
    "X = pd.get_dummies(tic_tac_toe.iloc[:,0:9])\n",
    "y = tic_tac_toe.iloc[:, 9]\n",
    "\n",
    "# Create training and testing datasets. Use 10% for the test set\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.1, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two holdout sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary training and final testing datasets\n",
    "X_temp, X_test, y_temp, y_test  =\\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have training, validation, and testing datasets, but do you know when you need both validation and testing datasets?\n",
    "\n",
    "When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics for Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mae](mae.png)\n",
    "\n",
    "![mse](mse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Manually calculate the MAE\n",
    "n = len(predictions)\n",
    "mae_one = sum(abs(y_test - predictions)) / n\n",
    "print('With a manual calculation, the error is {}'.format(mae_one))\n",
    "\n",
    "# Use scikit-learn to calculate the MAE\n",
    "mae_two = mean_absolute_error(y_test, predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mae_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a manual calculation, the error is 5.9\n",
    "\n",
    "Using scikit-lean, the error is 5.9\n",
    "\n",
    "These predictions were about six wins off on average. This isn't too bad considering NBA teams play 82 games a year. Let's see how these errors would look if you used the mean squared error instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "\n",
    "If you use the MAE, this accuracy metric does not reflect the bad predictions as much as if you use the MSE. Squaring the large errors from bad predictions will make the accuracy look worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n = len(predictions)\n",
    "# Finish the manual calculation of the MSE\n",
    "mse_one = sum((y_test - predictions)**2) / n\n",
    "print('With a manual calculation, the error is {}'.format(mse_one))\n",
    "\n",
    "# Use the scikit-learn function to calculate MSE\n",
    "mse_two = mean_squared_error(y_test, predictions)\n",
    "print('Using scikit-lean, the error is {}'.format(mse_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a manual calculation, the error is 49.1\n",
    "\n",
    "Using scikit-lean, the error is 49.1\n",
    "\n",
    "If you run any additional models, you will try to beat an MSE of 49.1, which is the average squared error of using your model. Although the MSE is not as interpretable as the MAE, it will help us select a model that has fewer 'large' errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on data subsets\n",
    "\n",
    "In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their own conference will do this year.\n",
    "\n",
    "You have been working on an NBA prediction model and would like to determine if the predictions were better for the East or West conference. You added a third array to your data called labels, which contains an \"E\" for the East teams, and a \"W\" for the West. y_test and predictions have again been loaded for your use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the East conference teams\n",
    "east_teams = labels == \"E\"\n",
    "\n",
    "# Find the East conference teams\n",
    "east_teams = labels == \"E\"\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "true_east = y_test[east_teams]\n",
    "preds_east = predictions[east_teams]\n",
    "\n",
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(\n",
    "    mae(true_east, preds_east)))\n",
    "\n",
    "# Print the West accuracy\n",
    "print('The MAE for West conference is {}'.format(west_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE for East teams is 6.733333333333333\n",
    "\n",
    "The MAE for West conference is 5.01\n",
    "\n",
    "It looks like the Western conference predictions were about two games better on average. \n",
    "Over the past few seasons, the Western teams have generally won the same number of games as the experts have predicted. \n",
    "Teams in the East are just not as predictable as those in the West."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![classification_metrics](classification_metrics.png)\n",
    "\n",
    "![confusion_matrix](confusion_matrix.png)\n",
    "\n",
    "![precision](precision.png)\n",
    "Precision - no. of true positives predicted out of the predicted positives.\n",
    "\n",
    "Used when we don't want to over predict positive values.\n",
    "\n",
    "![recall](recall.png)\n",
    "Recall - no. of true positives predicted out of all positives in the dataset.\n",
    "\n",
    "Used when we can't afford to miss any positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "Confusion matrices are a great way to start exploring your model's accuracy. They provide the values needed to calculate a wide range of metrics, including sensitivity, specificity, and the F1-score.\n",
    "\n",
    "You have built a classification model to predict if a person has a broken arm based on an X-ray image. On the testing set, you have the following confusion matrix:\n",
    "\n",
    "|          |  Prediction: 0    | Prediction: 1 |\n",
    "|----------|-------------------|---------------|\n",
    "|Actual: 0 |\t324 (TN)\t   |  15 (FP)      |\n",
    "|----------|-------------------|---------------|\n",
    "|Actual: 1 |\t123 (FN)\t   |  491 (TP)     |\n",
    "\n",
    "- Use the confusion matrix to calculate overall accuracy.\n",
    "\n",
    "- Use the confusion matrix to calculate precision and recall.\n",
    "\n",
    "- Use the three print statements to print each accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the accuracy\n",
    "accuracy = (324  + 491 ) / (953)\n",
    "print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print the precision\n",
    "precision = (491 ) / (15  + 491 )\n",
    "print(\"The precision is {0: 0.2f}\".format(precision))\n",
    "\n",
    "# Calculate and print the recall\n",
    "recall = (491 ) / (123  + 491 )\n",
    "print(\"The recall is {0: 0.2f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy is  0.86\n",
    "\n",
    "The precision is  0.97\n",
    "\n",
    "The recall is  0.80\n",
    "\n",
    "In this case, a true positive is a picture of an actual broken arm that was also predicted to be broken. Doctors are okay with a few additional false positives (predicted broken, not actually broken), as long as you don't miss anyone who needs immediate medical attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you read about confusion matrices on another website or for another programming language, the values might be reversed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create predictions\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print(\"The number of true positives is: {}\".format(cm[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[177 123]\n",
    "\n",
    " [ 92 471]]\n",
    " \n",
    "The number of true positives is: 471\n",
    "\n",
    "Row 1, column 1 represents the number of actual 1s that were predicted 1s (the true positives). Always make sure you understand the orientation of the confusion matrix before you start using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision VS Recall\n",
    "\n",
    "The accuracy metrics you use to evaluate your model should always be based on the specific application. For this example, let's assume you are a really sore loser when it comes to playing Tic-Tac-Toe, but only when you are certain that you are going to win.\n",
    "\n",
    "Choose the most appropriate accuracy metric, either precision or recall, to complete this example. But remember, if you think you are going to win, you better win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create precision or recall score based on the metric you imported\n",
    "score = precision_score(y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "print(\"The precision value is {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision value is 0.79\n",
    "\n",
    "Precision is the correct metric here. Sore-losers can't stand losing when they are certain they will win! For that reason, our model needs to be as precise as possible. With a precision of only 79%, you may need to try some other modeling techniques to improve this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "![variance](variance.png)\n",
    "\n",
    "![variance_overfitting](variance_overfitting.png)\n",
    "\n",
    "Easier to identify because the Training error will be a lot lower than the testing error.\n",
    "\n",
    "![bias](bias.png)\n",
    "\n",
    "![bias_underfitting](bias_underfitting.png)\n",
    "\n",
    "Occurs when model could not find underlying patterns in the data.\n",
    "\n",
    "The model attaches meaning to the noise in the training data.\n",
    "\n",
    "More difficult to identify because the training and testing errors will both be high.\n",
    "\n",
    "Difficult to know if we got the most out of the data and if we can improve the test error.\n",
    "\n",
    "![optimal_performance](optimal_performance.png)\n",
    "\n",
    "Model getting the most out of the training data and still performing in the testing data.\n",
    "\n",
    "**Compare how the model has performed on the data it has seen to the data it has not seen (train error and test error)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error due to under/over-fitting\n",
    "\n",
    "Too many features can lead to overfitting.\n",
    "\n",
    "The candy dataset is prime for overfitting. With only 85 observations, if you use 20% for the testing dataset, you are losing a lot of vital data that could be used for modeling. Imagine the scenario where most of the chocolate candies ended up in the training data and very few in the holdout sample. Our model might only see that chocolate is a vital factor, but fail to find that other attributes are also important. In this exercise, you'll explore how using too many features (columns) in a random forest model can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set max_features to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=2)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is 3.88\n",
    "\n",
    "The testing error is 9.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change max_features to 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=11)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is 3.57\n",
    "\n",
    "The testing error is 10.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change max_features to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25,\n",
    "                            random_state=1111,\n",
    "                            max_features=4)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and testing accuracies \n",
    "print('The training error is {0:.2f}'.format(\n",
    "  mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(\n",
    "  mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is 3.60\n",
    "\n",
    "The testing error is 8.79\n",
    "\n",
    "The chart below shows the performance at various max feature values. Sometimes, setting parameter values can make a huge difference in model performance.\n",
    "\n",
    "![chart_max_features](chart_max_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Am I underfitting?\n",
    "\n",
    "You are creating a random forest model to predict if you will win a future game of Tic-Tac-Toe. Using the tic_tac_toe dataset, you have created training and testing datasets, X_train, X_test, y_train, and y_test.\n",
    "\n",
    "You have decided to create a bunch of random forest models with varying amounts of trees (1, 2, 3, 4, 5, 10, 20, and 50). The more trees you use, the longer your random forest model will take to run. However, if you don't use enough trees, you risk underfitting. You have created a for loop to test your model at the different number of trees.\n",
    "\n",
    "- For each loop, predict values for both the X_train and X_test datasets.\n",
    "- For each loop, append the accuracy_score() of the y_train dataset and the corresponding predictions to train_scores.\n",
    "- For each loop, append the accuracy_score() of the y_test dataset and the corresponding predictions to test_scores.\n",
    "- Print the training and testing scores using the print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_scores, train_scores = [], []\n",
    "for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
    "    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Create predictions for the X_train and X_test datasets.\n",
    "    train_predictions = rfc.predict(X_train)\n",
    "    test_predictions = rfc.predict(X_test)\n",
    "    # Append the accuracy score for the test and train predictions.\n",
    "    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
    "    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
    "# Print the train and test scores.\n",
    "print(\"The training scores were: {}\".format(train_scores))\n",
    "print(\"The testing scores were: {}\".format(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training scores were: [0.94, 0.93, 0.98, 0.97, 0.99, 1.0, 1.0, 1.0]\n",
    "    \n",
    "The testing scores were: [0.83, 0.79, 0.89, 0.91, 0.91, 0.93, 0.97, 0.98]\n",
    "    \n",
    "Notice that with only one tree, both the train and test scores are low. As you add more trees, both errors improve. Even at 50 trees, this still might not be enough. Every time you use more trees, you achieve higher accuracy. At some point though, more trees increase training time, but do not decrease testing error.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with hold out sets\n",
    "\n",
    "The split matters.\n",
    "\n",
    "Moreso when the dataset is small.\n",
    "\n",
    "- Using different data splitting methods may lead to varying data in the final holdout samples.\n",
    "- If you have limited data, your holdout accuracy may be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two different samples of 200 observations \n",
    "sample1 = tic_tac_toe.sample(200, random_state=1111)\n",
    "sample2 = tic_tac_toe.sample(200, random_state=1171)\n",
    "\n",
    "# Print the number of common observations \n",
    "print(len([index for index in sample1.index if index in sample2.index]))\n",
    "\n",
    "# Print the number of observations in the Class column for both samples \n",
    "print(sample1['Class'].value_counts())\n",
    "print(sample2['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40\n",
    "\n",
    "positive    134\n",
    "\n",
    "negative     66\n",
    "\n",
    "Name: Class, dtype: int64\n",
    "\n",
    "\n",
    "positive    123\n",
    "\n",
    "negative     77\n",
    "\n",
    "Name: Class, dtype: int64\n",
    "\n",
    "Notice that there are a varying number of positive observations for both sample test sets. Sometimes creating a single test holdout sample is not enough to achieve the high levels of model validation you want. You need to use something more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "![cross_validation](cross_validation.png)\n",
    "\n",
    "The above diagram represents 5 fold cross validation. We'll evaluate the model on 5 different validation sets making us more confident in our model and splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn's KFold()\n",
    "\n",
    "Splits contain training and validation indices for the differents splits not train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Use KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1111)\n",
    "\n",
    "# Create splits\n",
    "splits = kf.split(X)\n",
    "\n",
    "# Print the number of indices\n",
    "for train_index, val_index in splits:\n",
    "    print(\"Number of training indices: %s\" % len(train_index))\n",
    "    print(\"Number of validation indices: %s\" % len(val_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of training indices: 68\n",
    "    \n",
    "Number of validation indices: 17\n",
    "    \n",
    "Number of training indices: 68\n",
    "    \n",
    "Number of validation indices: 17\n",
    "    \n",
    "Number of training indices: 68\n",
    "    \n",
    "Number of validation indices: 17\n",
    "    \n",
    "Number of training indices: 68\n",
    "    \n",
    "Number of validation indices: 17\n",
    "    \n",
    "Number of training indices: 68\n",
    "    \n",
    "Number of validation indices: 17\n",
    "    \n",
    "This dataset has 85 rows. You have created five splits - each containing 68 training and 17 validation indices. You can use these indices to complete 5-fold cross-validation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-fold indices\n",
    "\n",
    "You have already created splits, which contains indices for the candy-data dataset to complete 5-fold cross-validation. To get a better estimate for how well a colleague's random forest model will perform on a new data, you want to run this model on the five different training and validation indices you just created.\n",
    "\n",
    "- Use train_index and val_index to call the correct indices of X and y when creating training and validation data.\n",
    "\n",
    "- Fit rfc using the training dataset\n",
    "\n",
    "- Use rfc to create predictions for validation dataset and print the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "\n",
    "# Access the training and validation indices of splits\n",
    "for train_index, val_index in splits:\n",
    "    # Setup the training and validation data\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_val, y_val = X[val_index], y[val_index]\n",
    "    # Fit the random forest model\n",
    "    rfc.fit(X_train, y_train)\n",
    "    # Make predictions, and print the accuracy\n",
    "    predictions = rfc.predict(X_val)\n",
    "    print(\"Split accuracy: \" + str(mean_squared_error(y_val, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split accuracy: 178.75586448813047\n",
    "    \n",
    "Split accuracy: 98.29560208158634\n",
    "    \n",
    "Split accuracy: 86.2673010849621\n",
    "    \n",
    "Split accuracy: 217.4185114496197\n",
    "    \n",
    "Split accuracy: 140.5437661156536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold() is a great method for accessing individual indices when completing cross-validation. One drawback is needing a for loop to work through the indices though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn's cross_val_score()\n",
    "\n",
    "![cross_val_score](cross_val_score.png)\n",
    "\n",
    "By default, cross_val_score will use the default scoring method for the estimator defined. To change the scoring method use sklearn's make_score function.\n",
    "![scoring](scoring.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction 1: Load the cross-validation method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instruction 2: Load the random forest regression model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instruction 3: Load the mean squared error method\n",
    "# Instruction 4: Load the function for creating a scorer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see how all of the methods can get mixed up, but it is important to know the names of the methods you need. \n",
    "\n",
    "You can always review the scikit-learn documentation should you need any help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your company has created several new candies to sell, but they are not sure if they should release all five of them. To predict the popularity of these new candies, you have been asked to build a regression model using the candy dataset. Remember that the response value is a head-to-head win-percentage against other candies.\n",
    "\n",
    "Before you begin trying different regression models, you have decided to run cross-validation on a simple random forest model to get a baseline error to compare with any future results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(n_estimators=25, random_state=1111)\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "# Set up cross_val_score\n",
    "cv = cross_val_score(estimator=rfc,\n",
    "                     X=X_train,\n",
    "                     y=y_train,\n",
    "                     cv=10,\n",
    "                     scoring=mse)\n",
    "\n",
    "# Print the mean error\n",
    "print(cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "155.55845080026586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a baseline score to build on. If you decide to build additional models or try new techniques, you should try to get an error lower than 155.56. Lower errors indicate that your popularity predictions are improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "![loocv](loocv.png)\n",
    "\n",
    "Every single observation in the data will be used as a validation set. The rest of the points are used for training.\n",
    "\n",
    "![when_to_use_loocv](when_to_use_loocv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume your favorite candy is not in the candy dataset, and that you are interested in the popularity of this candy. Using 5-fold cross-validation will train on only 80% of the data at a time. The candy dataset only has 85 rows though, and leaving out 20% of the data could hinder our model. However, using leave-one-out-cross-validation allows us to make the most out of our limited dataset and will give you the best estimate for your favorite candy's popularity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# Create scorer\n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=15, random_state=1111)\n",
    "\n",
    "# Implement LOOCV\n",
    "scores = cross_val_score(estimator=rfr, X=X, y=y, cv=len(X), scoring=mae_scorer)\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "print(\"The mean of the errors is: %s.\" % np.mean(scores))\n",
    "print(\"The standard deviation of the errors is: %s.\" % np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the errors is: 9.464989603398694.\n",
    "    \n",
    "The standard deviation of the errors is: 7.265762094853885.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Hyperparameter Tuning\n",
    "\n",
    "![model_parameters](model_parameters.png)\n",
    "\n",
    "The coefficients and intercepts in a Linear Regression model are examples of model parameters. \n",
    "\n",
    "![linear_regression_model_parameters](linear_regression_model_parameters.png)\n",
    "\n",
    "Note that these model parameters were only created after we called .fit on the model. Otherwise they would not be created.\n",
    "\n",
    "![linear_regression_model_parameters_not_fitted](linear_regression_model_parameters_not_fitted.png)\n",
    "\n",
    "![hyperparameters](hyperparameters.png)\n",
    "\n",
    "Example is the alpha in a regularized linear regression model. It specifies the regularization strength.\n",
    "\n",
    "![hyperparameter_tuning](hyperparameter_tuning.png)\n",
    "\n",
    "Challenges:\n",
    "\n",
    "- Selecting the right hyperparameters to tune.\n",
    "\n",
    "- Specifying the appropriate values ranges for each hyperparameter.\n",
    "\n",
    "![practical_tuning_guidelines](practical_tuning_guidelines.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Hyperparameters\n",
    "For a school assignment, your professor has asked your class to create a random forest model to predict the average test score for the final exam.\n",
    "\n",
    "After developing an initial random forest model, you are unsatisfied with the overall accuracy. You realize that there are too many hyperparameters to choose from, and each one has a lot of possible values. You have decided to make a list of possible ranges for the hyperparameters you might use in your next model.\n",
    "\n",
    "Your professor has provided de-identified data for the last ten quizzes to act as the training data. There are 30 students in your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the parameters of rfr\n",
    "print(rfr.get_params())\n",
    "\n",
    "# Maximum Depth\n",
    "max_depth = [4, 8, 12]\n",
    "\n",
    "# Minimum samples for a split\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Maximum Features\n",
    "max_features = [4, 6, 8, 10]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Fill in rfr using your variables\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=random.choice(max_depth),\n",
    "    min_samples_split=random.choice(min_samples_split),\n",
    "    max_features=random.choice(max_features))\n",
    "\n",
    "# Print out the parameters\n",
    "print(rfr.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Model Validation with Hyperparameter Tuning\n",
    "\n",
    "![grid_search](grid_search.png)\n",
    "\n",
    "![grid_search_pro_con](grid_search_pro_con.png)\n",
    "\n",
    "![better_tuning_methods](better_tuning_methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n",
    "![random_search_parameters](random_search_parameters.png)\n",
    "\n",
    "![random_search_cv](random_search_cv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for RandomizedSearch\n",
    "Last semester your professor challenged your class to build a predictive model to predict final exam test scores. You tried running a few different models by randomly selecting hyperparameters. However, running each model required you to code it individually.\n",
    "\n",
    "After learning about RandomizedSearchCV(), you're revisiting your professors challenge to build the best model. In this exercise, you will prepare the three necessary inputs for completing a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Finish the dictionary by adding the max_depth parameter\n",
    "param_dist = {\"max_depth\": [2, 4, 6, 8],\n",
    "              \"max_features\": [2, 4, 6, 8, 10],\n",
    "              \"min_samples_split\": [2, 4, 8, 16]}\n",
    "\n",
    "# Create a random forest regression model\n",
    "rfr = RandomForestRegressor(n_estimators=10, random_state=1111)\n",
    "\n",
    "# Create a scorer to use (use the mean squared error)\n",
    "scorer = make_scorer(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing RandomizedSearchCV\n",
    "You are hoping that using a random search algorithm will help you improve predictions for a class assignment. You professor has challenged your class to predict the overall final exam average score.\n",
    "\n",
    "In preparation for completing a random search, you have created:\n",
    "\n",
    "- param_dist: the hyperparameter distributions\n",
    "- rfr: a random forest regression model\n",
    "- scorer: a scoring method to use\n",
    "\n",
    "Instructions    \n",
    "\n",
    "- Load the method for conducting a random search in sklearn.\n",
    "- Complete a random search by filling in the parameters: estimator, param_distributions, and scoring.\n",
    "- Use 5-fold cross validation for this random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the method for random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Build a random search using param_dist, rfr, and scorer\n",
    "random_search =\\\n",
    "    RandomizedSearchCV(\n",
    "        estimator=rfr,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=5,\n",
    "        scoring=scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it takes a lot of steps, hyperparameter tuning with random search is well worth it and can improve the accuracy of your models. Plus, you are already using cross-validation to validate your best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting your final model\n",
    "\n",
    "### Explore key attributes of a fitted RandomizedSearchCV algorithm\n",
    "\n",
    "![random_search_attributes](random_search_attributes.png)\n",
    "\n",
    "![random_search_other_attributes](random_search_other_attributes.png)\n",
    "\n",
    "![using_cv_results](using_cv_results.png)\n",
    "\n",
    "![best_estimator](best_estimator.png)\n",
    "\n",
    "![using_best_estimator](using_best_estimator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, make_scorer\n",
    "\n",
    "# Create a precision scorer\n",
    "precision = make_scorer(precision_score)\n",
    "# Finalize the random search\n",
    "rs = RandomizedSearchCV(\n",
    "  estimator=rfc, param_distributions=param_dist,\n",
    "  scoring = precision,\n",
    "  cv=5, n_iter=10, random_state=1111)\n",
    "rs.fit(X, y)\n",
    "\n",
    "# print the mean test scores:\n",
    "print('The accuracy for each run was: {}.'.format(rs.cv_results_['mean_test_score']))\n",
    "# print the best model score:\n",
    "print('The best accuracy for a single model was: {}'.format(rs.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for each run was: [0.86446668 0.75302055 0.67570816 0.88459939 0.88381178 0.86917588\n",
    " 0.68014695 0.81721906 0.87895856 0.92917474].\n",
    "    \n",
    "The best accuracy for a single model was: 0.9291747446879924\n",
    "    \n",
    "Your model's precision was 93%! The best model accurately predicts a winning game 93% of the time. If you look at the mean test scores, you can tell some of the other parameter sets did really poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
